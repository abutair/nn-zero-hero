{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size) :\n",
    "        \n",
    "        self.input_size= input_size\n",
    "        self.hidden_size= hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.layers = self.init_params()\n",
    "\n",
    "    def init_params(self):\n",
    "\n",
    "        np.random.seed(0)\n",
    "\n",
    "        # scaling factor \n",
    "        k= 1/math.sqrt(self.hidden_size)\n",
    "        # Initialize weights and biases\n",
    "        i_weight = np.random.rand(self.input_size,self.hidden_size)*2*k-k \n",
    "        h_weight = np.random.rand(self.hidden_size, self.hidden_size) * 2 * k - k\n",
    "        h_bias = np.random.rand(1, self.hidden_size) * 2 * k - k\n",
    "        o_weight = np.random.rand(self.hidden_size, self.output_size) * 2 * k - k\n",
    "\n",
    "        o_bias = np.random.rand(1, self.output_size) * 2 * k - k\n",
    "        return [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        i_weight, h_weight, h_bias, o_bias = self.layers\n",
    "        hidden = np.zeros((len(x),self.hidden_size))\n",
    "        output=  np.zeros((len(x),self.output_size))\n",
    "\n",
    "        \n",
    "        for j in range(len(x)):\n",
    "            # Compute input to hidden layer\n",
    "            input_x = x[j] @ i_weight\n",
    "            # Compute hidden state (using previous hidden state)\n",
    "            hidden_x = input_x + hidden[max(j-1,0)] @ h_weight + h_bias\n",
    "            # Apply tanh activation function\n",
    "            hidden_x = np.tanh(hidden_x)\n",
    "            hidden[j] = hidden_x\n",
    "            # Compute output\n",
    "            output_x = hidden_x @ o_weight + o_bias\n",
    "            # Apply softmax to get probabilities\n",
    "            output[j] = np.exp(output_x) / np.sum(np.exp(output_x))\n",
    "        \n",
    "        return hidden, output\n",
    "    \n",
    "    def backward(self, x, grad, hidden, lr):\n",
    "        i_weight, h_weight, h_bias, o_weight, o_bias = self.layers\n",
    "        next_h_grad = None\n",
    "        i_weight_grad, h_weight_grad, h_bias_grad, o_weight_grad, o_bias_grad = [0] * 5 \n",
    "\n",
    "        for j in range(len(x)-1,-1,-1):\n",
    "            out_grad = grad[j][np.newaxis, :]\n",
    "            o_weight_grad += hidden[j][np.newaxis, :].T @ out_grad\n",
    "            o_bias_grad += out_grad\n",
    "            h_grad = out_grad @ o_weight.T\n",
    "\n",
    "            if j<len(x)-1:\n",
    "                hh_grad= next_h_grad @ h_weight.T\n",
    "                h_grad += h_grad \n",
    "\n",
    "            tanh_deriv = 1- hidden[j][np.newaxis,:] **2\n",
    "            h_grad = np.multiply(h_grad, tanh_deriv)\n",
    "            next_h_grad = h_grad.copy()\n",
    "\n",
    "            if j > 0:\n",
    "                h_weight_grad += hidden[j-1][np.newaxis, :].T @ h_grad\n",
    "                h_bias_grad += h_grad\n",
    "            i_weight_grad += x[j][np.newaxis, :].T @ h_grad\n",
    "\n",
    "            lr= lr/len(x)\n",
    "            i_weight -=i_weight_grad * lr\n",
    "            h_weight -= i_weight_grad * lr\n",
    "            h_bias -= h_bias_grad*lr\n",
    "            o_weight -= o_weight_grad * lr\n",
    "            o_bias -= o_bias_grad * lr\n",
    "\n",
    "            self.layers = [i_weight,h_weight,h_bias,o_weight, o_bias]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'([])'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"([{}])\"\n",
    "s = s.replace('()', '').replace('[]', '').replace('{}', '')\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from datasets) (1.26.3)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp39-cp39-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from datasets) (0.24.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\abutair\\appdata\\roaming\\python\\python39\\site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from aiohttp->datasets) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\abutair\\appdata\\roaming\\python\\python39\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\abutair\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\abutair\\appdata\\roaming\\python\\python39\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\abutair\\workspace\\nn-zero-to-hero\\.conda\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\abutair\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-3.0.2-py3-none-any.whl (472 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "Downloading pyarrow-17.0.0-cp39-cp39-win_amd64.whl (25.1 MB)\n",
      "   ---------------------------------------- 0.0/25.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 1.8/25.1 MB 10.1 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 4.7/25.1 MB 11.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 7.3/25.1 MB 12.2 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.4/25.1 MB 12.4 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 8.7/25.1 MB 8.5 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 14.4/25.1 MB 11.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 16.8/25.1 MB 11.9 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.6/25.1 MB 12.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.6/25.1 MB 12.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.6/25.1 MB 12.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 18.6/25.1 MB 12.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.1/25.1 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.1/25.1 MB 9.5 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
      "Successfully installed datasets-3.0.2 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset with streaming to avoid downloading the entire dataset\n",
    "dataset = load_dataset(\"riotu-lab/ARABIC-RAW-TEXT\", split='train', streaming=True)\n",
    "\n",
    "# Extract the first 10,000 rows\n",
    "sample_data = list(dataset.take(10000))\n",
    "\n",
    "# Save the sample data to a text file\n",
    "with open(\"arabic_raw_sample.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in sample_data:\n",
    "        f.write(row['text'] + \"\\n\")\n",
    "\n",
    "print(\"Sample data saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
